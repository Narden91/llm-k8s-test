apiVersion: v1
kind: Pod
metadata:
  name: llm-chat
  namespace: default
  labels:
    app: llm-chat
spec:
  restartPolicy: Always
  runtimeClassName: seeweb-nvidia-1xa6000
  imagePullSecrets:
    - name: ghcr-secret
  containers:
  - name: llm-streamlit
    image: "ghcr.io/narden91/llm-chat:latest"
    imagePullPolicy: Always
    ports:
    - containerPort: 8501
      name: streamlit
    resources:
      limits:
        nvidia.com/gpu: "1"
      requests:
        memory: "16Gi"
        cpu: "4"
    env:
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: llm-secrets
          key: HF_TOKEN
          optional: true
    - name: CUDA_VISIBLE_DEVICES
      value: "0"
    envFrom:
      - configMapRef:
          name: llm-configmap
          optional: true
    volumeMounts:
    - name: hf-cache
      mountPath: /root/.cache/huggingface
    livenessProbe:
      httpGet:
        path: /_stcore/health
        port: 8501
      initialDelaySeconds: 300
      periodSeconds: 30
      timeoutSeconds: 10
    readinessProbe:
      httpGet:
        path: /_stcore/health
        port: 8501
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
  volumes:
  - name: hf-cache
    emptyDir:
      sizeLimit: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: llm-chat-service
  namespace: default
spec:
  selector:
    app: llm-chat
  ports:
  - port: 8501
    targetPort: 8501
    name: streamlit
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-configmap
  namespace: default
data:
  MODEL_ID: "mistralai/Mistral-7B-Instruct-v0.3"
  GPU_MEMORY_UTILIZATION: "0.9"
  MAX_TOKENS: "2048"
  TEMPERATURE: "0.7"
