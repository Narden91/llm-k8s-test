# LLM Chat Application Dependencies
# Optimized for vLLM on NVIDIA A6000

# Core ML/DL - use pip index for CUDA wheels
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.1.2

# vLLM (includes its own torch dependency management)
vllm==0.4.2

# LLM utilities
transformers>=4.40.0
accelerate>=0.27.0
huggingface-hub>=0.22.0

# Configuration
pydantic>=2.0.0
pyyaml>=6.0.0

# Web interface
streamlit>=1.32.0

# Utilities
rich>=10.0.0
python-dotenv>=1.0.0

