# LLM Chat Application Dockerfile
# Using official vLLM image as base for NVIDIA A6000

FROM vllm/vllm-openai:v0.4.2

LABEL maintainer="Emanuele Nardone"
LABEL description="LLM Chat Application with Streamlit and vLLM"

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

WORKDIR /app

# Install additional dependencies (vLLM base already has torch, transformers, etc.)
RUN pip install --no-cache-dir \
    streamlit>=1.32.0 \
    pydantic>=2.0.0 \
    pyyaml>=6.0.0 \
    rich>=10.0.0 \
    python-dotenv>=1.0.0 \
    wandb>=0.16.0

# Copy application code
COPY llm_operations/ ./llm_operations/
COPY streamlit_app/ ./streamlit_app/
COPY configs/ ./configs/

# Expose Streamlit port
EXPOSE 8501

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:8501/_stcore/health || exit 1

# Run Streamlit (override the default vLLM entrypoint)
ENTRYPOINT []
CMD ["streamlit", "run", "streamlit_app/app.py", \
    "--server.address=0.0.0.0", \
    "--server.port=8501", \
    "--server.headless=true", \
    "--browser.gatherUsageStats=false"]
