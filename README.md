# ğŸ¤– LLM Chat Platform on Kubernetes

## ğŸ‘¤ Author
**Emanuele Nardone**

## ğŸ“Œ Overview
A **production-ready LLM inference platform** powered by **vLLM** and **Mistral 7B**, designed for deployment on **GPU-enabled Kubernetes clusters** (NVIDIA A6000). Features a modern **Streamlit chat interface** with streaming responses.

---

## âœ¨ Features

### ğŸ§  LLM Inference Engine
- **vLLM** for high-throughput, low-latency inference
- **Mistral 7B Instruct** with proper prompt formatting
- Configurable generation parameters (temperature, top-p, max tokens)
- Multi-turn conversation history management
- Streaming token generation for real-time responses

### ğŸ’¬ Chat Interface
- Modern **Streamlit** web UI
- Real-time streaming responses with typing indicator
- Configurable model settings via sidebar
- Session-based conversation management
- Mobile-responsive design

### ğŸ—ï¸ Infrastructure
- **NVIDIA CUDA** runtime optimized containers
- **Kubernetes** deployment with GPU scheduling
- Health checks and readiness probes
- GitHub Actions CI/CD pipeline
- Hugging Face model caching

---

## ğŸ“ Project Structure
```bash
llm-k8s-test/
â”œâ”€â”€ config/                   # ğŸ“œ Configuration handlers
â”‚   â””â”€â”€ s3_config_handler.py
â”œâ”€â”€ configs/                  # âš™ï¸ YAML configuration files
â”‚   â””â”€â”€ llm_config.yaml
â”œâ”€â”€ llm_operations/           # ğŸ§  LLM inference engine
â”‚   â”œâ”€â”€ llm_config.py         # Pydantic configuration models
â”‚   â””â”€â”€ llm_inference.py      # vLLM engine wrapper
â”œâ”€â”€ s3_operations/            # â˜ï¸ S3 storage utilities
â”‚   â”œâ”€â”€ s3_client.py          # Low-level S3 client wrapper
â”‚   â””â”€â”€ s3_operations.py      # High-level S3 operations
â”œâ”€â”€ streamlit_app/            # ğŸ’¬ Chat interface
â”‚   â””â”€â”€ app.py                # Streamlit application
â”œâ”€â”€ doc/                      # ğŸ“š Documentation
â”œâ”€â”€ .github/workflows/        # ğŸ”„ CI/CD pipelines
â”œâ”€â”€ Dockerfile.llm            # ğŸ³ LLM container image
â”œâ”€â”€ llm-manifest.yml          # â˜¸ï¸ Kubernetes deployment
â”œâ”€â”€ requirements-llm.txt      # ğŸ“¦ Python dependencies
â”œâ”€â”€ verify_s3.py              # ğŸ” S3 Setup Verification Script
â””â”€â”€ pyproject.toml            # ğŸ”§ Project configuration
```

---

## âš ï¸ Prerequisites
- **Kubernetes cluster** with NVIDIA GPU support
- **NVIDIA Container Toolkit** installed
- **NVIDIA A6000** (or compatible GPU with â‰¥16GB VRAM)
- **Hugging Face token** (optional, for gated models)
- **kubectl** CLI tool

---

## ğŸš€ Quick Start

### Local Development

This project uses [uv](https://github.com/astral-sh/uv) for fast Python package management.

```bash
# Install uv (Windows PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Install uv (macOS/Linux)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies
uv sync

# Run Streamlit app (requires GPU)
uv run streamlit run streamlit_app/app.py

# Verify S3 connection (optional)
uv run verify_s3.py
```

### Kubernetes Deployment

```bash
# Create GitHub Container Registry secret
kubectl create secret docker-registry ghcr-secret \
  --docker-server=ghcr.io \
  --docker-username=YOUR_GITHUB_USERNAME \
  --docker-password=YOUR_GITHUB_PAT \
  --docker-email=YOUR_GITHUB_EMAIL

# Create Hugging Face secret (optional, for gated models)
kubectl create secret generic llm-secrets \
  --from-literal=HF_TOKEN=your-hf-token

# Deploy the application
kubectl apply -f llm-manifest.yml

# Check pod status
kubectl get pods -l app=llm-chat

# View logs
kubectl logs -f -l app=llm-chat

# Port forward to access UI
kubectl port-forward svc/llm-chat-service 8501:8501
```

Then open http://localhost:8501 in your browser.

---

## ğŸ”§ Configuration

### Generation Parameters
| Parameter | Default | Description |
|-----------|---------|-------------|
| `temperature` | 0.7 | Sampling temperature (0-2). Higher = more creative |
| `max_tokens` | 2048 | Maximum response length |
| `top_p` | 0.95 | Nucleus sampling threshold |
| `top_k` | 50 | Top-k sampling |
| `repetition_penalty` | 1.1 | Token repetition penalty |

### Model Settings
| Parameter | Default | Description |
|-----------|---------|-------------|
| `model_id` | `mistralai/Mistral-7B-Instruct-v0.3` | Hugging Face model |
| `gpu_memory_utilization` | 0.9 | GPU memory fraction (0.1-0.99) |
| `tensor_parallel_size` | 1 | Number of GPUs for tensor parallelism |

---

## ğŸ”„ CI/CD Pipeline

The project includes GitHub Actions workflows for automated container builds:

- **Triggers** on version tags (`v*.*.*`)
- **Builds** optimized vLLM-based container image
- **Pushes** to GitHub Container Registry (GHCR)
- **Tags** with semantic version and Git SHA

### Triggering a Build
```bash
git tag v1.0.0
git push origin v1.0.0
```

---

## ğŸ” Monitoring & Troubleshooting

### Check Pod Status
```bash
kubectl get pods -l app=llm-chat
kubectl describe pod -l app=llm-chat
```

### View Logs
```bash
kubectl logs -f -l app=llm-chat
```

### Check GPU Status
```bash
kubectl exec -it $(kubectl get pods -l app=llm-chat -o jsonpath='{.items[0].metadata.name}') -- nvidia-smi
```

### Common Issues

| Issue | Solution |
|-------|----------|
| Model loading timeout | Increase `initialDelaySeconds` in readiness probe |
| OOM errors | Reduce `gpu_memory_utilization` or use smaller model |
| Slow first response | Normal - KV cache warmup on first request |

---

## ğŸ“š Documentation

Additional documentation available in the `doc/` folder:
- [CLI Reference](doc/CLI_REFERENCE.md)
- [Kubernetes Commands](doc/K8s_commands.md)
- [Seeweb Setup Guide](doc/Seeweb_setup_guide.md)

### Future Project Ideas
- [Multi-Model GPU Orchestration](doc/IDEA_MULTI_MODEL_ORCHESTRATION.md)
- [Federated LLM with Privacy](doc/IDEA_FEDERATED_LLM_PRIVACY.md)
- [Self-Optimizing LLM Inference](doc/IDEA_SELF_OPTIMIZING_LLM.md)

---

## ğŸ“œ License
**Unicas & Seeweb**
